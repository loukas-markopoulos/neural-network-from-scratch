{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create classes that defines a layer and the functions to compute the forward and backwards pass through the layer. These will be connected later to form the neural network.\n",
    "\n",
    "This is shown in the following image:\n",
    "\n",
    "![neural network](neural_network.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes output y of a layer for a given input x\n",
    "    def forward_pass(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and updates the parameters)\n",
    "    def backward_pass(self, output_gradient, learning_rate):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within each layer, there is a dense layer and an acitvation layer. These will be created seperately for ease of understanding how a layer works. The activation layer step could be skipped by directly implementing the activation function inside the dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dense layer, the forward and backwards propogation functions for each layer must be defined.\n",
    "\n",
    "Forwards propogation will simply use y = w.x + b to calculate the Y value of that layer (often called the Z value). This value will later be used with an activation function to find the A value.\n",
    "\n",
    "The following equations will be used to find the derivatives necesary to calculate backwards propogation:\n",
    "\n",
    "![DERIV_EQ](deriv_eq.png)\n",
    "\n",
    "where E is the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dense layer where:\n",
    "# input_size = number of input neurons\n",
    "# output_size = number of output neurons\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size)\n",
    "        self.bias = np.random.randn(1, output_size)\n",
    "    \n",
    "    # returns the output for a given input\n",
    "    def forward_pass(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.input, self.weights) + self.bias\n",
    "    \n",
    "    # computes dE/dW, dE/dB for a given output_error (dE/dY)\n",
    "    # returns input_error = dE/dX\n",
    "    def backward_pass(self, output_gradient, learning_rate):\n",
    "        # output_gradient is the derivative of the error with respect to the biases\n",
    "        # calculating derivative of error with respect to weights\n",
    "        input_error = np.dot(output_gradient, self.weights.T)\n",
    "        weights_gradient = np.dot(self.input.T, output_gradient)\n",
    "        # updating parameters\n",
    "        self.weights -= np.multiply(weights_gradient, learning_rate)\n",
    "        self.bias -= np.multiply(output_gradient, learning_rate)\n",
    "        # return the dertivative of the error with respect to the input\n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, an activation layer must be created after the dense layer. Creating these seperately makes the model simpler to understand. In the activation layer, each input value goes to a different, single neuron in the acitvation layer (instead of each input value going to all the neurons in a dense layer). This is simply because each Y value in the dense layer has the same function performed on them. \n",
    "\n",
    "The forward_pass function will give the output of the whole layer- the activation value.\n",
    "\n",
    "We want to find the derivative of the error with respect to the input so that it can be minimised by changing the w and b parameters. The following equation shows that function:\n",
    "![activ_backpass](activ_backpass.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_deriv):\n",
    "        self.activation = activation\n",
    "        self.activation_deriv = activation_deriv\n",
    "    \n",
    "    # returns the acitvated value of the layer\n",
    "    def forward_pass(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "    \n",
    "    # returns the input_error (dE/dX) for a given output_error (dE/dY)\n",
    "    # no use of learning rate since no parameters are being updated\n",
    "    def backward_pass(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_deriv(self.input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the neural network needs to be created by making a network class which will allow the user to make a desired netowrk of any size using the acitvation functions from activation_funcs.ipynb and  the mean squared error function from error_funcs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_deriv = None\n",
    "    \n",
    "    # function to allow user to add a layer to the network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    # function to set loss to use\n",
    "    def use(self, loss, loss_deriv):\n",
    "        self.loss = loss\n",
    "        self.loss_deriv = loss_deriv\n",
    "    \n",
    "    # function to predict an output for a given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        features, samples = input_data.shape\n",
    "        result = []\n",
    "\n",
    "        # run network over all the input samples\n",
    "        for i in range(samples):\n",
    "            # forward propogation\n",
    "            output = input_data[:, i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_pass(output)\n",
    "                result.append(output)\n",
    "        \n",
    "        # return the result array that contains all the outputs for each input\n",
    "        return result\n",
    "\n",
    "    # train the network using the training data set\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        features, samples = x_train.shape\n",
    "    \n",
    "        # loop over all the training samples\n",
    "        for i in range(epochs):\n",
    "            error = 0\n",
    "            for j in range(samples):\n",
    "                # forward propogation\n",
    "                output = x_train[:, j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_pass(output)\n",
    "                \n",
    "                # compute loss (for display purposes only)\n",
    "                error += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propogation to update the parameters w and b\n",
    "                error = self.loss_deriv(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_pass(error, learning_rate)\n",
    "                \n",
    "                # calculate the average error on all of the samples\n",
    "                error /= samples\n",
    "                print(f'epoch {i + 1}{epochs}, error = {error}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network assumes that the input data is a matrix where each column is an individual data point and each row is a different feature for the data points.\n",
    "\n",
    "An example of this can be seen in data.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
