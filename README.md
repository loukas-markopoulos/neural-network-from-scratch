# Neural Network Construction Library

This library allows users to build, train, and test their own neural network architecture.
Relu, softmax, and tanh activation functions can be used for each layer of any desired number of neurons.

Forward propogation, backward propogation, activation function and a mean squared error function have been implemented from scratch.
Comments throughout the jupyter notebooks describe the mathematics behind each function.


I have also tested the model on Yahoo stock prices between 2015 - 2021.
stock_predictor.ipynb show how to use the model.
The model returned the following output:
## ![Predicted vs Actual Output](https://github.com/user-attachments/assets/43de3822-2f52-4fbb-9457-f9cc403f494f)
